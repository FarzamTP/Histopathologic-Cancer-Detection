{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib.pyplot import savefig\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./dataset/train/\"\n",
    "TEST_PATH = \"./dataset/test/\"\n",
    "IMG_WIDTH, IMG_HEIGHT = 96, 96\n",
    "# Between 1 and 220025\n",
    "NUMBER_OF_SAMPLES = 200000\n",
    "BATCH_SIZE = 4\n",
    "FILE_NAME = str(int(NUMBER_OF_SAMPLES / 1000)) + f'K-{IMG_WIDTH}x{IMG_HEIGHT}.npy'\n",
    "LOAD_MATRICES = False\n",
    "USE_GENERATOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/train_labels.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if null\n",
    "We should make sure that there are no `null` fields in the dataset.\n",
    "If there were any:\n",
    "* Remove data row\n",
    "* Fill according to other data (Which can not be done here, as the data are images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_empty_id_rows = len(df[df.id.isnull()])\n",
    "number_of_empty_label_rows = len(df[df.label.isnull()])\n",
    "\n",
    "if number_of_empty_id_rows == 0:\n",
    "    print(\"[NOTE]: Dataset 'id' column has no 'Null' values. No need to fill/remove rows.\")\n",
    "if number_of_empty_label_rows == 0:\n",
    "    print(\"[NOTE]: Dataset 'label' column has no 'Null' values. No need to fill/remove rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dataset\n",
    "\n",
    "* Sampling positive and negative samples will help us understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tumor_samples = df[df.label == 1]\n",
    "number_of_posive_tumor_samples = len(positive_tumor_samples)\n",
    "number_of_posive_tumor_samples\n",
    "print(f\"Number of positive tumor samples: {number_of_posive_tumor_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tumor_samples = df[df.label == 0]\n",
    "number_of_negative_tumor_samples = len(negative_tumor_samples)\n",
    "number_of_negative_tumor_samples\n",
    "print(f\"Number of negative tumor samples: {number_of_negative_tumor_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Information provided by [Kaggle](https://www.kaggle.com/c/histopathologic-cancer-detection/data)\n",
    "\n",
    "In this dataset, you are provided with a large number of small pathology images to classify. Files are named with an image id. The `train_labels.csv` file provides the ground truth for the images in the train folder. You are predicting the labels for the images in the test folder. ***A positive label indicates that the center `32x32px` region of a patch contains at least one pixel of tumor tissue.*** Tumor tissue in the outer region of the patch ***does not*** influence the label. This outer region is provided to enable fully-convolutional models that do not use zero-padding, to ensure consistent behavior when applied to a whole-slide image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "fig.suptitle('Histopathologic scans of lymph node sections',fontsize=20)\n",
    "\n",
    "ax[0, 0].set_ylabel(\"Normal Tissue Samples\", size='large')\n",
    "ax[1, 0].set_ylabel(\"Positive Tumor Samples\", size='large')\n",
    "\n",
    "for i in range(5):\n",
    "    # Negative Tumor Tissue Sample\n",
    "    random_negative_sample = negative_tumor_samples.iloc[random.randint(0, number_of_negative_tumor_samples)]\n",
    "    random_negative_sample_id = random_negative_sample.id\n",
    "    random_negative_sample_label = random_negative_sample.label\n",
    "    random_negative_sample_path = os.path.join(TRAIN_PATH, random_negative_sample_id) + '.tif'\n",
    "    random_negative_img = cv2.imread(random_negative_sample_path)\n",
    "    random_negative_img = cv2.cvtColor(random_negative_img, cv2.COLOR_BGR2RGB)\n",
    "    rect = patches.Rectangle((32, 32), 32, 32, linewidth=3, edgecolor='b', facecolor='none')\n",
    "    ax[0, i].add_patch(rect)\n",
    "    ax[0, i].imshow(random_negative_img)\n",
    "    \n",
    "    # Positive Tumor Tissue Samples\n",
    "    random_positive_sample = positive_tumor_samples.iloc[random.randint(0, number_of_posive_tumor_samples)]\n",
    "    random_positive_sample_id = random_positive_sample.id\n",
    "    random_positive_sample_label = random_positive_sample.label\n",
    "    random_positive_sample_path = os.path.join(TRAIN_PATH, random_positive_sample_id) + '.tif'\n",
    "    random_positive_img = cv2.imread(random_positive_sample_path)\n",
    "    random_positive_img = cv2.cvtColor(random_positive_img, cv2.COLOR_BGR2RGB)\n",
    "    rect = patches.Rectangle((32, 32), 32, 32, linewidth=3, edgecolor='r', facecolor='none')\n",
    "    ax[1, i].imshow(random_positive_img)\n",
    "    ax[1, i].add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_convert_images_from_df(dataframe, base_dir):\n",
    "    img_list = []\n",
    "    img_label_list = []\n",
    "    total_number_of_data = NUMBER_OF_SAMPLES\n",
    "    for index, img_datum in dataframe.iterrows():\n",
    "        if index < NUMBER_OF_SAMPLES:\n",
    "            print(f\"{index + 1} out of {total_number_of_data}\")\n",
    "            print(f\"Image ID: {img_datum.id}, Label: {img_datum.label}\")\n",
    "            img_id = img_datum.id\n",
    "            img_label = df[df.id == img_id].iloc[0].label\n",
    "            img_path = os.path.join(base_dir, img_id) + '.tif'\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "            img = np.asarray(img, dtype=float)\n",
    "            img /= 255.0\n",
    "            img_list.append(img)\n",
    "            img_label_list.append(img_label)\n",
    "    return img_list, img_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_convert_images_from_path(path):\n",
    "    img_list = []\n",
    "    img_id_list = []\n",
    "    for idx, img_name in enumerate(os.listdir(path)):\n",
    "        print(f\"{idx + 1} out of {NUMBER_OF_SAMPLES}\")\n",
    "        img_id = img_name.split(\".tif\")[0]\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        img = np.asarray(img, dtype=float)\n",
    "        img /= 255.0\n",
    "        img_list.append(img)\n",
    "        img_id_list.append(img_id)\n",
    "    img_list = np.asarray(img_list, dtype='float')\n",
    "    img_id_list = np.asarray(img_id_list)\n",
    "    return img_list, img_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrices():\n",
    "    X = np.load(f'./matrices/X_{FILE_NAME}')\n",
    "    y = np.load(f'./matrices/y_{FILE_NAME}')\n",
    "    \n",
    "    X_eval = np.load('./matrices/X_EVAL.npy')\n",
    "    ID_eval = np.load('./matrices/ID_EVAL.npy')\n",
    "    \n",
    "    return X, y, X_eval, ID_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    if LOAD_MATRICES:\n",
    "        X, y, X_eval, ID_eval = load_matrices()\n",
    "        print(f\"X Shape: {X.shape}\")\n",
    "        print(f\"y Shape: {y.shape}\")\n",
    "        print(f\"X eval Shape: {X_eval.shape}\")\n",
    "        print(f\"ID eval Shape: {ID_eval.shape}\")\n",
    "    else:\n",
    "        X, y = read_convert_images_from_df(df, TRAIN_PATH)\n",
    "\n",
    "        X_eval, ID_eval = read_convert_images_from_path(TEST_PATH)\n",
    "\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        np.save(f'./matrices/X_{FILE_NAME}', X)\n",
    "        np.save(f'./matrices/y_{FILE_NAME}', y)\n",
    "\n",
    "        np.save('./matrices/X_EVAL.npy', X_eval)\n",
    "        np.save('./matrices/ID_EVAL.npy', ID_eval)\n",
    "\n",
    "        print(\"X saved!\")\n",
    "        print(\"y saved!\")\n",
    "\n",
    "        print(\"X eval saved!\")\n",
    "        print(\"ID eval saved!\")\n",
    "\n",
    "        print(f\"X Shape: {X.shape}\")\n",
    "        print(f\"y Shape: {y.shape}\")\n",
    "\n",
    "        print(f\"X eval Shape: {X_eval.shape}\")\n",
    "        print(f\"ID eval Shape: {ID_eval.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    print(f\"X Train shape: {x_train.shape}\\ny train Shape: {x_train.shape}\\nX Test shape: {x_test.shape}\\ny test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.InputLayer(input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), name='input_layer'),\n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=L2(0.001)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.MaxPool2D((3, 3), padding='same'),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=L2(0.001)),\n",
    "    layers.MaxPool2D((3, 3), padding='same'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.MaxPool2D((3, 3), padding='same'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "], name='Simple_CNN_16x16_200K_Tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    history = model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), batch_size=256, epochs=50, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test):\n",
    "    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    y_pred_eval = predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history, model_name): \n",
    "    os.mkdir(f'./plots/{model_name}')\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(f'./plots/{model_name}/acc.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(f'./plots/{model_name}/loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not USE_GENERATOR:\n",
    "    plot_loss_acc(history, model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_classes_for_generator(base_dir, df):\n",
    "    for index, img_datum in df.iterrows():\n",
    "        if index < 60000:\n",
    "            dir_name = 'valid'\n",
    "        else:\n",
    "            dir_name = 'train'\n",
    "            \n",
    "        img_id = img_datum.id\n",
    "        img_label = df[df.id == img_id].iloc[0].label\n",
    "        img_path = os.path.join(base_dir, img_id) + '.tif'\n",
    "        img_destination = f\"./dataset/Splitted/{dir_name}/{str(img_label)}/{img_id}.tif\"\n",
    "        print(f\"Copying {index + 1} out of {220000}\")\n",
    "        copyfile(img_path, img_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_classes_for_generator(TRAIN_PATH, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './dataset/Splitted/train/',\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        './dataset/Splitted/valid/',\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "\n",
    "generator_model = Sequential(name=\"Image_Data_Generator_Model\")\n",
    "generator_model.add(layers.Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "generator_model.add(layers.Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "generator_model.add(layers.Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "generator_model.add(layers.MaxPooling2D(pool_size = pool_size)) \n",
    "generator_model.add(layers.Dropout(dropout_conv))\n",
    "\n",
    "generator_model.add(layers.Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "generator_model.add(layers.Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "generator_model.add(layers.Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "generator_model.add(layers.MaxPooling2D(pool_size = pool_size))\n",
    "generator_model.add(layers.Dropout(dropout_conv))\n",
    "\n",
    "generator_model.add(layers.Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "generator_model.add(layers.Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "generator_model.add(layers.Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "generator_model.add(layers.MaxPooling2D(pool_size = pool_size))\n",
    "generator_model.add(layers.Dropout(dropout_conv))\n",
    "\n",
    "generator_model.add(layers.Flatten())\n",
    "generator_model.add(layers.Dense(256, activation = \"relu\"))\n",
    "generator_model.add(layers.Dropout(dropout_dense))\n",
    "generator_model.add(layers.Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./models/model.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = generator_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(160025 / BATCH_SIZE),\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(60000 / BATCH_SIZE),\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history, generator_model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./models/KIDG_50_epochs.h5\")\n",
    "print(\"Pre-trained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57458 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        TEST_PATH,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14365/14365 [==============================] - 49s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "probabilities = model.predict(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_probabilities_to_classes(probabilities):\n",
    "    predicted_classes = []\n",
    "    for prob in probabilities:\n",
    "        if prob[0] > 0.5:\n",
    "            predicted_classes.append(1)\n",
    "        else:\n",
    "            predicted_classes.append(0)\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = convert_probabilities_to_classes(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_filenames_to_img_id(filenames):\n",
    "    for idx, img_path in enumerate(filenames):\n",
    "        filenames[idx] = img_path.split(\".\")[0].split(\"/\")[1]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = convert_filenames_to_img_id(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_submit_csv_file(predicted_classes, filenames):\n",
    "    if os.path.exists('./submission.csv'):\n",
    "        !rm submission.csv\n",
    "        print(\"Removed old version.\")\n",
    "\n",
    "    with open('./submission.csv', 'w+') as f:\n",
    "        f.write(\"id,label\\n\")\n",
    "        for idx, prediction in enumerate(predicted_classes):\n",
    "            f.write(f'{filenames[idx]},{prediction}\\n')\n",
    "        print(\"Submission.csv file created!\")\n",
    "        !./submit_csv.sh Image-Data-Generator\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c603db21156b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_and_submit_csv_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_classes' is not defined"
     ]
    }
   ],
   "source": [
    "create_and_submit_csv_file(predicted_classes, filenames)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
